### MOTHS ### 

Extra notes:
- before meeting:
  - increase cursor size (settings-> accessibility). 
  - drag pop up to side to see audience
- YouthBoard begin and end year. 
- Practice the start
  - Write out. 
- Ambitions:
 - write out 3 looking and 3 bring. 
 - practice this job
 - link to the job

  Don't touch your hair. 

  # existing literature:
  In the time I had to prepare this presentation, I delved into research that has already been done and saw that mothector.
  Of course, I don't know exactly how things stand, but these are some possible ideas from me

  we want to know -> they want to know. Don't say you're on the project. 
  bittensor was a big challenge -> bittensor is a big challenge. 

  After the skills. 
  I would find it a great challenge to take part in this amazing project. 
  I see it as an excellent opportunity to share my knowledge.

- https://lepmon.de/en/lepmon-en/ # LAPMON Webpage
- https://inf-cv.uni-jena.de/home/publications/  # Research Group Computer Vision Jena
- https://arxiv.org/pdf/2307.15427 # Encoutered existing paper: Deep Learning Pipeline for Automated Visual Moth Monitoring: Insect Localization and Species Classification


Read FIDO before: Explaining image classifiers by counterfactual generation Chang et al. (2018) -> https://arxiv.org/pdf/1807.08024
https://link.springer.com/article/10.1007/s11263-025-02453-z
Simplified Concrete Dropout - Improving the Generation of Attribution Masks for Fine-grained Classification
Interpretation: What if 2 species look very similar?
problem is that most Attribution methods (attention, gradient) not able to display relevant regsions as accurate (scale up, vs. irrelevant areas) 


FIDO = fill-in of the dropout

Fine-Grained = classification with small and subtle differences

attribution mask = probablity map that teaches which pixels to keep/drop

What is the smallest pixel set that preserves the decision?

Variance issue: drop critical pixel -> big loss, keeping it, no loss. Noisy gradients. Originally solved by big batch sizes, this paper: reduces variance, cleaner masks (30 steps and 2 batch sizes: 4 and 16). 

3 CNNS: 
- ResNet50
- Inveption V3
- ConvNeXt-B

Concrete dropout:
 - iteratively optimize the mask probabilities (which means the pixels explanatory for distinguing 2 fine grained species (to keep/drop ))
 - initialize mask probabilities (α(x,y), each pizel 0.5)
 - first iteration random sampling mask (some 0 and some 1). Apply to image, feed into frozen classifier, Compute loss + sparsity → backprop to α
 - update α after 1 step. wing pixels become 0.7 while background 0.3. 
 - Second iteration, mask samples, but but more background pixels dropped. Repeat until convergence means: probablities regions (α) almost 1 and non probable 0, 
   and mask becomes certain which pixels to drop (0) and keep (1) 
 - Finally you compare the 3 models inference class prediction capabilities of providing the full image vs. the mask 


often same object parts: 
SSR = smallest region that retains a certain classification score (good prediction +)
SDR = smallest region that minimizes the target classification when this region is changed (makes prediction most bad - )

Simplified Concrete Dropout (Korsch et al., 2025): "transformer-based approaches(He et al., 2022; Yu et a., 2022) are currently at the top in different fine-grained classification benchmarks."

moth species (Rodner et al., 2015)

Rodner, E., Simon, M., Brehm, G., Pietsch, S., Wägele, J.W., Denzler,
J.( 2015). Fine-grained recognition datasets for biodiversity analysis. In: CVPR Workshop on Fine-grained Visual Classification
(CVPR-WS)

φ(x, z) = (1 − z)  x + z  xˆ ,
apply a mask to x and the opposite to xˆ
 d


# Prepare papers:
# slide build up on existing research
- 1. transformer architectures
    - Transformers for image recognition: https://arxiv.org/pdf/2010.11929, AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE
    - RT-DETR: https://arxiv.org/abs/2304.08069
- 2. NLP support
    - CLIP = Contrastive Language-Image Pre-training, https://arxiv.org/pdf/2103.00020, Learning Transferable Visual Models From Natural Language Supervision
    - Cascaded Vision Language Models https://arxiv.org/pdf/2405.11301
- 3. Interpretation and pixel masking
    - FIDO (Fill-In the DropOut): https://arxiv.org/pdf/1807.08024, used a generative model to generate in-fills pixels consistent with the surroundings. 
      - https://github.com/zphang/saliency_investigation
    - Simplified Concrete Dropout: https://arxiv.org/pdf/2307.14825, Improving the Generation of Attribution Masks for Fine-grained Classification

